{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from math import exp\n",
    "from math import log\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "  for l in urllib.urlopen(fname):\n",
    "    yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dataFile = open(\"C:/Users/BHEL/Desktop/Recommendation Systems/Assignment 1/winequality-white.csv\")\n",
    "header = dataFile.readline()\n",
    "fields = [\"constant\"] + header.strip().replace('\"','').split(';')\n",
    "featureNames = fields[:-1]\n",
    "labelName = fields[-1]\n",
    "lines = [[1.0] + [float(x) for x in l.split(';')] for l in dataFile]\n",
    "X = [l[:-1] for l in lines]\n",
    "y = [l[-1] > 5 for l in lines]\n",
    "\n",
    "def inner(x,y):\n",
    "  return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1.0 / (1 + exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Logistic regression by gradient ascent         #\n",
    "##################################################\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "  loglikelihood = 0\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    loglikelihood -= log(1 + exp(-logit))\n",
    "    if not y[i]:\n",
    "      loglikelihood -= logit\n",
    "  for k in range(len(theta)):\n",
    "    loglikelihood -= lam * theta[k]*theta[k]\n",
    "  # for debugging\n",
    "  # print \"ll =\", loglikelihood\n",
    "  return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "  dl = [0]*len(theta)\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    for k in range(len(theta)):\n",
    "      dl[k] += X[i][k] * (1 - sigmoid(logit))\n",
    "      if not y[i]:\n",
    "        dl[k] -= X[i][k]\n",
    "  for k in range(len(theta)):\n",
    "    dl[k] -= lam*2*theta[k]\n",
    "  return numpy.array([-x for x in dl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = X[:int(len(X)/3)]\n",
    "y_train = y[:int(len(y)/3)]\n",
    "X_validate = X[int(len(X)/3):int(2*len(X)/3)]\n",
    "y_validate = y[int(len(y)/3):int(2*len(y)/3)]\n",
    "X_test = X[int(2*len(X)/3):]\n",
    "y_test = y[int(2*len(X)/3):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Train                                          #\n",
    "##################################################\n",
    "\n",
    "def train(lam):\n",
    "  theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, pgtol = 10, args = (X_train, y_train, lam))  \n",
    "  return theta\n",
    "\n",
    "##################################################\n",
    "# Predict                                        #\n",
    "##################################################\n",
    "\n",
    "def performance(theta):\n",
    "\n",
    "  scores_test = [inner(theta,x) for x in X_test]\n",
    "  predictions_test = [s > 0 for s in scores_test]\n",
    "  tp = [(a==1 and b==1) for (a,b) in zip(predictions_test,y_test)]\n",
    "  tn = [(a==0 and b==0) for (a,b) in zip(predictions_test,y_test)]\n",
    "  fp = [(a==1 and b==0) for (a,b) in zip(predictions_test,y_test)]\n",
    "  fn = [(a==0 and b==1) for (a,b) in zip(predictions_test,y_test)]\n",
    "\n",
    "  #acc_test = sum(correct_test) * 1.0 / len(correct_test)\n",
    "  return sum(tp),sum(tn),sum(fp),sum(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives=1129; True Negatives=145; False Positives=321; False Negatives=38; Balanced Error Rate=0.360701663412\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Validation pipeline                            #\n",
    "##################################################\n",
    "theta = train(0.01)\n",
    "tp,tn,fp,fn = performance(theta)\n",
    "tpr = tp/(tp+fn)\n",
    "tnr = tn/(tn+fp)\n",
    "ber = 1 - (1/2)*(tpr+tnr)\n",
    "print(\"True Positives=\" + str(tp) + \"; True Negatives=\" + str(tn) + \"; False Positives=\" + str(fp) + \"; False Negatives=\" + str(fn) + \\\n",
    "    \"; Balanced Error Rate=\" + str(ber) )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
